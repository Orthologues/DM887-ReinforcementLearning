\documentclass[a4paper,12pt,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\DeclareMathOperator{\argmax}{arg,max}
\DeclareMathOperator{\argmin}{arg,min}
\setlength\parindent{0pt}
\setlength{\topmargin}{-2cm}
\setlength{\textheight}{24cm}

\hoffset= -70pt
\textwidth=540pt
\numberwithin{equation}{section}

\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{xcolor}
\usepackage{lmodern} % Load a font package


\begin{document}

\title{DM887 Assignment2 Q1}
\author{Jiawei Zhao}
\date{19.03.2024}
\maketitle
\title {Implementation of Least-Squares Temporal Differences (LSTD) Deep Q-Learning with Nonlinear Feature Extraction that maps a state to lower-dimensional latent embedding. While the action-value function should be a linear function of the output of the feature extractor.}

\newcommand{\mycomment}[1]{{\fontfamily{lmss}\selectfont\textcolor{blue}{[#1]}}} % Sans-serif and blue
    \begin{algorithm}
        \begin{algorithmic}[1]
        \caption{Initialization}
        \State Initialize a variational autoencoder $\mathbf{A}$ as the feature extraction network with initial weights \(w_0\)
        \mycomment {\(w_0\) are drawn from Glorot uniform initialization}
        \State Initialize the learning rate of $\mathbf{A}$ as $\alpha=1 \times 10^{-2}$ 
        \State Initialize an Adam optimizer \(o\) with a learning rate $\alpha$ for $\mathbf{A}$
        \State Initialize the minibatch size of DQN as \(k=32\) 
        \State Initialize the total number of training episodes \(N\) = 500, the number of episodes at each cycle consisting of three phases of training $N_0=50$, and the number of episodes at each seperate phase of training \(N_1\) = 10 
        \mycomment {It would be a wise choice to seperate the warm-up phase, the antoencoder update phase, and the LSTD update phase at each cycle}
        \State Initialize the number of maximum time step per episode \(T\) = 1000
        \State Initialize the weights $\theta_0$ for linear approximation function randomly between $(0, 1)$ which will be used for LSTD
        \State Initialize a replay memory buffer $\mathcal{D}$ with a capacity \(N \times T\)
        \State Initialize a discount factor $\gamma=0.9$ 
        \State Initialize a small constant $\lambda=1 \times 10^{-3}$ to initialize $A^{-1}$ for LSTD
        \State Given the number of actions as \(N_a\) and embedding dimension \(N_e\), initialize a matrix $\theta$ with shape $N_a \times N_e$
        \State Initialize a tensor $A_a^{-1} = \lambda^{-1}I$ and a tensor $b_a=0$ to store the tensors that suffice $\theta_{a} = A_a^{-1} b_a$, $\forall a \in A$ at each time step $t$
        \mycomment{A relatively large discount factor encourages long-term planning and faster convergence during training}
        \end{algorithmic}[1]
    \end{algorithm}
        
    \clearpage
    \begin{algorithm}
    \caption{Warm-up phase}
        \begin{algorithmic}[1]
            \State Freeze the weights of \(f(phi(s))\), i.e. $\theta$
            \State Freeze the weights of $\mathbf{A}$, i.e. \(w\)
        \For {\(episode\) \(e_0 = 1\) to \(N_1\)}
            \State Initialize state \(s\) 
            \State Preprocess \(s\) into \(s_0\) to adapt it as input of $\mathbf{A}$
            \mycomment{preprocessing of high-dimensional states is necessary w.r.t. autoenconders}
            \For {each time step \(t\) = 1 to \(T\)}
                \State Encode state \(s_t\) using $\mathbf{A}$ to get latent embedding $\phi(s_t)$
                \State Select action $a_t$ using $\epsilon$-greedy policy with $\epsilon=0.2$ with an $\epsilon$ decay
                \State Execute $a_t$ and obtain reward $r_t$ and new state $s_{t+1}$
                \If{$s_{t+1} \notin S$}
                    \State \textbf{break}
                \EndIf
                \State Store the transition of the current \(t\), i.e. $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{D}$
            \EndFor
        \EndFor
        \State $e=e+N_1$
        \end{algorithmic}[1]
    \end{algorithm}
    
    \clearpage
    \begin{algorithm}
    \caption{Autoencoder update phase}
        \begin{algorithmic}[1]
            \State Freeze the weights of \(f(phi(s))\), i.e. $\theta$
            \State Unfreeze the weights of $\mathbf{A}$, i.e. \(w\)
            \State Reset \(A_a^{-1}\) and \(b_a\) to the default value at the initialization phase, $\forall a \in A$ at each time step $t$
        \For {\(episode\) \(e_0 = 1\) to \(N_1\)}
            \State Repeat the same steps at the warm-up phase
            \For {each time step \(t\) = 1 to \(T\)}
                \State Repeat the same steps at the warm-up phase
                \\
                \mycomment{Start updating the autoencoder using minibatches}
                \State Sample a minibatch of transitions \(d\) from $\mathcal{D}$ with a batchsize \(k\) \
                \State Use the \(s_t\) of \(d\) as input to $\mathbf{A}$
                \State First encode, then decode \(d\) using $\mathbf{A}$
                \State Calculate the loss \(L\) of \(s_t\) by comparing the input and output of $\mathbf{A}$
                \State Update \(w\) with \(o\) as per \(L\)
                \
            \EndFor
        \EndFor
        \State $e=e+N_1$
        \end{algorithmic}[1]
    \end{algorithm}

    \clearpage
    \begin{algorithm}
    \caption{LSTD update phase}
        \begin{algorithmic}[1]
            \State Freeze the weights of \(f(phi(s))\), i.e. $\theta$
            \State Unfreeze the weights of $\mathbf{A}$, i.e. \(w\)
        \For {\(episode\) \(e_0 = 1\) to \(N_1\)}
            \State Repeat the same steps at the warm-up phase
            \For {each time step \(t\) = 1 to \(T\)}
                \State Repeat the same steps at the warm-up phase
                \\
                \mycomment{Start using the online LSTD algorithm to update $\theta$}
                \State Calculate $\tau = \phi(s_t) - \gamma \phi(s_{t+1})$
                \State Calculate $v = \tau^{T} A^{-1}$
                \State Update $A^{-1} = A^{-1} - \frac{A^{-1} \phi(s) v^{T}}{1 + v \phi(s)}$
                \State Update $b = b + r \phi(s)$
                \State Given the action $a$ of the current time step, update $\theta_a = A^{-1} b $
                \State Update state $s_t=s_{t+1}$
            \EndFor
        \EndFor
        \State $e=e+N_1$
        \end{algorithmic}[1]    
    \end{algorithm}


    \clearpage
    \begin{algorithm}
        \caption{Training procedure}
        \begin{algorithmic}[1]
        \State Run \(Initialization\)
        \While {\(episode\) $e \leq N$}
            \State Run \(\textit{Warm-up phase}\)
            \State Run \(\textit{Autoencoder update phase}\)
            \State Run \(\textit{LSTD update phase}\)
            \State Run \(\textit{Autoencoder update phase}\)
            \If{$e + N_1 \geq N $}
                \State Reset \(A_a^{-1}\) and \(b_a\) to the default value at the initialization phase, $\forall a \in A$ at each time step $t$
            \EndIf
            \State Run \(\textit{LSTD update phase}\)
        \EndWhile
        \end{algorithmic}[1]    
    \end{algorithm}
    
    
\end{document}