\documentclass[a4paper,12pt,oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\DeclareMathOperator{\argmax}{arg,max}
\DeclareMathOperator{\argmin}{arg,min}
\setlength\parindent{0pt}
\setlength{\topmargin}{-2cm}
\setlength{\textheight}{24cm}

\hoffset= -70pt
\textwidth=540pt
\numberwithin{equation}{section}

\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{xcolor}
\usepackage{lmodern} % Load a font package


\begin{document}

\title{DM887 Assignment2 Q1}
\author{Jiawei Zhao}
\date{19.03.2024}
\maketitle
\title {Implementation of Least-Squares Temporal Differences (LSTD) Deep Q-Learning with Nonlinear Feature Extraction that maps a state to lower-dimensional latent embedding. While the action-value function should be a linear function of the output of the feature extractor.}

\newcommand{\mycomment}[1]{{\fontfamily{lmss}\selectfont\textcolor{blue}{[#1]}}} % Sans-serif and blue
    \begin{algorithm}
        \begin{algorithmic}[1]
        \caption{Initialization}
        \State Initialize a variational autoencoder $\mathbf{A}$ as the feature extraction network with initial weights \(w_0\)
        \mycomment {\(w_0\) are drawn from Glorot uniform initialization}
        \State Initialize the learning rate of $\mathbf{A}$ as $\alpha$ 
        \State Initialize an Adam optimizer \(o\) with a learning rate $\alpha$ for $\mathbf{A}$
        \State Initialize the minibatch size of DQN as \(k\) 
        \State Initialize the total number of training episodes \(N\), the number of episodes at each cycle consisting of three phases of training $N_0$, ensure that $N \mod N_0 = 0$
        \State Set the number of episodes at the warm-up phase as \(N_1\), at the autoencoder update phase as \(N_2\), and at the LSTD weight update phase as \(N_3\), ensure that $(N_0-N_1) \mod (N2+N3) = 0$
        \mycomment {It would be a wise choice to seperate the warm-up phase, the antoencoder update phase, and the LSTD update phase at each cycle}
        \State Initialize the number of maximum time step per episode \(T\)
        \State Initialize the weights $\theta_0$ for linear approximation function randomly between $(0, 1)$ which will be used for LSTD
        \State Initialize a replay memory buffer $\mathcal{D}$ with a capacity \(N \times T\)
        \State Initialize a relatively discount factor, e.g. $\gamma = 0.9$ 
        \State Initialize a small constant $\lambda$, e.g. $1 \times 10^{-3}$ to initialize $A^{-1}$ for LSTD
        \State Given the number of actions as \(N_a\) and embedding dimension \(N_e\), initialize a matrix $\theta$ with shape $N_a \times N_e$
        \State Initialize a tensor $A^{-1} = \lambda^{-1}I$ with shape $N_a \times N_e \times N_e $ and a tensor $b$ with shape $N_a \times N_e$ full of zeros to store the tensors that suffice $\theta_{a} = A_a^{-1} b_a$, $\forall a \in A$
        \mycomment{A relatively large discount factor encourages long-term planning and faster convergence during training}
        \end{algorithmic}[1]
    \end{algorithm}
        
    \clearpage
    \begin{algorithm}
    \caption{Warm-up phase}
        \begin{algorithmic}[1]
            \State Freeze the weights of \(f(phi(s))\), i.e. $\theta$
            \State Freeze the weights of $\mathbf{A}$, i.e. \(w\)
        \For {\(episode\) \(e_0 = 1\) to \(N_1\)}
            \State Initialize state \(s\) 
            \State Preprocess \(s\) into \(s_0\) to adapt it as input of $\mathbf{A}$
            \mycomment{preprocessing of high-dimensional states is necessary w.r.t. autoenconders}
            \For {each time step \(t\) = 1 to \(T\)}
                \State Encode state \(s_t\) using $\mathbf{A}$ to get latent embedding $\phi(s_t)$
                \State Select action $a_t$ using $\epsilon$-greedy policy with $\epsilon=0.2$ with an $\epsilon$ decay
                \State Execute $a_t$ and obtain reward $r_t$ and new state $s_{t+1}$
                \If{$s_{t+1} \notin S$}
                    \State \textbf{break}
                \EndIf
                \State Store the transition of the current \(t\), i.e. $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{D}$
            \EndFor
        \EndFor
        \State $e=e+N_1$
        \end{algorithmic}[1]
    \end{algorithm}
    
    \clearpage
    \begin{algorithm}
    \caption{Autoencoder update phase}
        \begin{algorithmic}[1]
            \State Freeze the weights of \(f(phi(s))\), i.e. $\theta$
            \State Unfreeze the weights of $\mathbf{A}$, i.e. \(w\)
        \For {\(episode\) \(e_0 = 1\) to \(N_2\)}
            \State Repeat the same steps at the warm-up phase
            \For {each time step \(t\) = 1 to \(T\)}
                \State Repeat the same steps at the warm-up phase
                \\
                \mycomment{Start updating the autoencoder using minibatches}
                \State Sample a minibatch of transitions \(d\) from $\mathcal{D}$ with a batchsize \(k\) \
                \State Use the \(s_t\) of \(d\) as input to $\mathbf{A}$
                \State First encode, then decode \(d\) using $\mathbf{A}$
                \State Calculate the loss \(L\) of \(s_t\) by comparing the input and output of $\mathbf{A}$
                \State Update \(w\) with \(o\) as per \(L\)
                \
            \EndFor
        \EndFor
        \State $e=e+N_2$
        \end{algorithmic}[1]
    \end{algorithm}

    \clearpage
    \begin{algorithm}
    \caption{LSTD update phase}
        \begin{algorithmic}[1]
            \State Freeze the weights of \(f(phi(s))\), i.e. $\theta$
            \State Unfreeze the weights of $\mathbf{A}$, i.e. \(w\)
        \For {\(episode\) \(e_0 = 1\) to \(N_3\)}
            \State Repeat the same steps at the warm-up phase
            \For {each time step \(t\) = 1 to \(T\)}
                \State Repeat the same steps at the warm-up phase
                \\
                \mycomment{Start using the online LSTD algorithm to update $\theta$}
                \State Calculate $\tau = \phi(s_t) - \gamma \phi(s_{t+1})$
                \State Calculate $v = \tau^{T} A^{-1}$
                \State Update $A_a^{-1} = A_a^{-1} - \frac{A_a^{-1} \phi(s) v^{T}}{1 + v \phi(s)}$
                \State Update $b_a = b_a + r \phi(s)$
                \State Given the action $a$ of the current time step, update $\theta_a = A_a^{-1} b_a $
                \State Update state $s_t=s_{t+1}$
            \EndFor
        \EndFor
        \State $e=e+N_3$
        \end{algorithmic}[1]    
    \end{algorithm}


    \clearpage
    \begin{algorithm}
        \caption{Training procedure}
        \begin{algorithmic}[1]
        \State Run \(Initialization\)
        \For {each training cycle $c = 1$ to $\frac{N}{N_0}$}
            \State Run \(\textit{Warm-up phase}\)
            \For {each intra-$c$ round $r=1$ to $\frac{N_0-N_1}{N_2+N_3}$}
            \State Run \(\textit{Autoencoder update phase}\)
            \State Reset $A^{-1}, b, \theta$ to the default value at the initialization phase
            \State Run \(\textit{LSTD update phase}\)
            \EndFor
        \EndFor
        \end{algorithmic}[1]    
    \end{algorithm}
    
    
\end{document}